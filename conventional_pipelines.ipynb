{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf0572b",
   "metadata": {},
   "source": [
    "# Political Toxicity: Conventional NLP Pipelines\n",
    "Using Apache Spark, the project should be able to take in CSV data from social media posts and classify them accurately as toxic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf7d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql import DataFrame\n",
    "spark = SparkSession.builder.appName('Test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9248b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, when\n",
    "\"\"\"\n",
    "Maps the column from string values to double values.\n",
    "0.0 in all unknown types or cases\n",
    "\"\"\"\n",
    "def map_toxic_hard(df, col_name=\"toxic_hard\", new_col=\"toxic_score\"):\n",
    "    return df.withColumn(\n",
    "        new_col,\n",
    "        when(trim(lower(col(col_name))) == \"true\", 1)   # string TRUE\n",
    "        .when(trim(lower(col(col_name))) == \"false\", 0) # string FALSE\n",
    "        .otherwise(0)                                   # everything else\n",
    "    )\n",
    "\n",
    "def preprocessing_steps(df: DataFrame) -> DataFrame:\n",
    "    temp = map_toxic_hard(df, \"toxic_hard\", \"toxic_score\")\n",
    "    return temp.select('text','toxic_score') \\\n",
    "        .dropna(subset=['text'])\n",
    "\n",
    "\"\"\"\n",
    "Reads the CSV containing the training data at the given path \n",
    "and returns a spark dataframe.\n",
    "\"\"\"\n",
    "def read_train_data(path:str) -> DataFrame:\n",
    "    seed_df = (\n",
    "        spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .option(\"multiline\", True)\n",
    "            .csv(path,sep=',',ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True)\n",
    "    )\n",
    "    seed_df = preprocessing_steps(seed_df)\n",
    "    return seed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2569a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train/dataset.csv\"\n",
    "seed_df = read_train_data(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cc5d2",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15806555",
   "metadata": {},
   "source": [
    "For all the tweets we need to remove all tokens that are not useful to the analysis. Then we need to create simple lists of relevant tokens for the unsupervised BERT model to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a66cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- toxic_score: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639e324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#from pyspark.ml import Transformer\n",
    "#from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71747329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline builder\n",
    "\n",
    "# ETL\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='cleaned_words')\n",
    "\n",
    "# Hash/Vectorize\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol='features')\n",
    "\n",
    "# Regression Estimation\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='toxic_score',\n",
    "        maxIter=10, regParam=0.001)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4782aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA T500\n"
     ]
    }
   ],
   "source": [
    "# Check whether the GPU is open\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddaaadb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_edc4db50033e"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" DEBUG\n",
    "# Print out all parameters for this stage\n",
    "print(\"LogisticRegression parameters:\")\n",
    "print(lr.explainParams())\n",
    "# If you want to check just the labelCol and featuresCol:\n",
    "print(\"Label column:\", lr.getLabelCol())\n",
    "print(\"Features column:\", lr.getFeaturesCol())\n",
    "\"\"\"\n",
    "# Now fit only on the required columns\n",
    "model = pipeline.fit(seed_df)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472527d7",
   "metadata": {},
   "source": [
    "## Predict on Test Data\n",
    "This will load a small test subset and confirm our predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e3a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"data/test/2pt_test.csv\"\n",
    "\n",
    "test_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(test_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "273fbbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- toxic_score_true: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da61616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------------------------------------+\n",
      "|toxic_score_true|prediction|probability                               |\n",
      "+----------------+----------+------------------------------------------+\n",
      "|0.0             |0.0       |[0.9985773268381002,0.0014226731618998123]|\n",
      "|1.0             |1.0       |[9.038140940731217E-5,0.9999096185905927] |\n",
      "+----------------+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)\n",
    "predictions.select('toxic_score_true','prediction','probability').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca5093",
   "metadata": {},
   "source": [
    "## Split Validation\n",
    "Repeat the pipeline building and analysis through split validation to demonstrate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c5ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16579c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='cleaned_words')\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='toxic_score',\n",
    "        maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,hashingTF,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10b8e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------+----------+\n",
      "|toxic_score_true|probability                              |prediction|\n",
      "+----------------+-----------------------------------------+----------+\n",
      "|0.0             |[0.8884556385439323,0.11154436145606772] |0.0       |\n",
      "|1.0             |[0.031650691973346586,0.9683493080266534]|1.0       |\n",
      "+----------------+-----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='toxic_score')\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,parallelism=10,\n",
    "    seed=SEED,\n",
    "    evaluator=evaluator,\n",
    "    estimatorParamMaps=grid)\n",
    "train_df = read_train_data(train_data_path)\n",
    "\n",
    "model = tvs.fit(train_df)\n",
    "model.bestModel.transform(dataset=test_df)\\\n",
    "    .select('toxic_score_true','probability','prediction')\\\n",
    "    .show(truncate=False)\n",
    "    # 'probability' represents the 1/P probability of that class \n",
    "    # being the predicted class of this document.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6f6d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.6995571644466337]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics\n",
    "# represents ROC AUC for each parameter setting tuple in the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773918d",
   "metadata": {},
   "source": [
    "### McNemar's Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e749d",
   "metadata": {},
   "source": [
    "Compares where models differ in classification to determine if there is a signifcant difference between them. If for example they both classify the same amount incorrectly then obviously there is no significant difference, but if one model is technically more accurate but they agree on most of the classifications, then the data will show the result is not significant enough to disprove the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ee7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model metrics\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def display_metrics(predictions):\n",
    "    # Assume 'predictions' DataFrame has:\n",
    "    #   - 'label' (0 or 1)\n",
    "    #   - 'prediction' (0 or 1)\n",
    "    #   - 'probability' (vector of probabilities, needed for ROC/AUC)\n",
    "\n",
    "    # --- Accuracy ---\n",
    "    acc_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    accuracy = acc_eval.evaluate(predictions)\n",
    "    print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    confusion = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "    confusion.show()\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # --- Precision, Recall, F1 ---\n",
    "    prec_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    rec_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    f1_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "\n",
    "    precision = prec_eval.evaluate(predictions)\n",
    "    recall = rec_eval.evaluate(predictions)\n",
    "    f1 = f1_eval.evaluate(predictions)\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # --- ROC AUC ---\n",
    "    auc_eval = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    roc_auc = auc_eval.evaluate(predictions)\n",
    "    print(\"ROC_AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cca988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataFrame\n",
    "train_df = read_train_data(train_data_path)\n",
    "text_col = 'text'\n",
    "label_col = 'toxic_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86894ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Word2Vec\n",
    "# Preprocessing steps for all pipelines\n",
    "tokenizer = Tokenizer(inputCol=text_col, outputCol=\"words\")\n",
    "filter = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='filtered_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5e8925da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF model\n",
    "hashing_tf = HashingTF(inputCol=filter.getOutputCol(), outputCol='raw_features')\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"features\")\n",
    "lr_tfidf = LogisticRegression(featuresCol=idf.getOutputCol(), labelCol=label_col,\n",
    "        maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_tfidf = Pipeline(stages=[tokenizer,filter,hashing_tf,idf,lr_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "084bbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"@realDailyWire It's time for the #Military #JAG Corp to arrest #JoeBiden for #HighTreason.  He has violated his oath of office to #Defend the #Constitution of the #UnitedStates of #America.  He's the head of the military, it's only fitting that he face #MilitaryJustice, #CourtMarshall.\", toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='@CBCAlerts F@ck it! Bring over the Taliban and all the Wuhan people who are scared of bats. Throw in a boat load from North Korea too! Go all out Trudeau‚Ä¶ need all the votes you can get. ü§°', toxic_score=1, pred_tfidf=1.0),\n",
       " Row(text=\"@AmberGDay @AJEnglish Who had reported this? Are you brainless ?, how can someone travel with children in a car full of ammunition?\\ndon't justify american terrorism\\n\\n#Afghanistan \\n#KabulAttack \\n#BloodyAmericanArmy\", toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='‚Ä¶ ‚ÄúThis includes using all of his oversight authorities and legal action if appropriate against governors who are trying to block and intimidate local schools officials and educators.‚Äù2//', toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='@darkmikasonfire @CHSommers Ok fascist bot. @twitter fascist bots are ok with you? But Nobel laureates and doctors are not? You are a fascist tool, that‚Äôs what you are.', toxic_score=1, pred_tfidf=1.0)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict on whole corpus (no holdout)\n",
    "model_tfidf = pipeline_tfidf.fit(train_df)\n",
    "prediction_raw = model_tfidf.transform(train_df)\n",
    "pred_tfidf = prediction_raw.select(text_col, label_col, col(\"prediction\").alias(\"pred_tfidf\"))\n",
    "pred_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53ddbe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 1.0\n",
      "--------------------------------------------------------\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 2989|\n",
      "|    1|       1.0|  587|\n",
      "+-----+----------+-----+\n",
      "\n",
      "--------------------------------------------------------\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "--------------------------------------------------------\n",
      "ROC_AUC Score: 0.9999994300510161\n"
     ]
    }
   ],
   "source": [
    "# Metrics Word2Vec\n",
    "display_metrics(prediction_raw.select(col(label_col).alias('label'), col('prediction'), col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "931e8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=0, inputCol=filter.getOutputCol(), outputCol=\"features\")\n",
    "lr_w2v = LogisticRegression(featuresCol=word2vec.getOutputCol(), labelCol=label_col)\n",
    "pipeline_w2v = Pipeline(stages=[tokenizer, filter, word2vec,lr_w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f408c3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"@realDailyWire It's time for the #Military #JAG Corp to arrest #JoeBiden for #HighTreason.  He has violated his oath of office to #Defend the #Constitution of the #UnitedStates of #America.  He's the head of the military, it's only fitting that he face #MilitaryJustice, #CourtMarshall.\", toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='@CBCAlerts F@ck it! Bring over the Taliban and all the Wuhan people who are scared of bats. Throw in a boat load from North Korea too! Go all out Trudeau‚Ä¶ need all the votes you can get. ü§°', toxic_score=1, pred_w2v=0.0),\n",
       " Row(text=\"@AmberGDay @AJEnglish Who had reported this? Are you brainless ?, how can someone travel with children in a car full of ammunition?\\ndon't justify american terrorism\\n\\n#Afghanistan \\n#KabulAttack \\n#BloodyAmericanArmy\", toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='‚Ä¶ ‚ÄúThis includes using all of his oversight authorities and legal action if appropriate against governors who are trying to block and intimidate local schools officials and educators.‚Äù2//', toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='@darkmikasonfire @CHSommers Ok fascist bot. @twitter fascist bots are ok with you? But Nobel laureates and doctors are not? You are a fascist tool, that‚Äôs what you are.', toxic_score=1, pred_w2v=0.0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict on WHOLE corpus (no holdout)\n",
    "model_w2v = pipeline_w2v.fit(train_df)\n",
    "prediction_raw = model_w2v.transform(train_df)\n",
    "pred_w2v = prediction_raw.select(text_col, label_col, col(\"prediction\").alias(\"pred_w2v\"))\n",
    "pred_w2v.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "882b30b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8364093959731543\n",
      "--------------------------------------------------------\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 2976|\n",
      "|    0|       1.0|   13|\n",
      "|    1|       0.0|  572|\n",
      "|    1|       1.0|   15|\n",
      "+-----+----------+-----+\n",
      "\n",
      "--------------------------------------------------------\n",
      "Precision: 0.7890338134943754\n",
      "Recall: 0.8364093959731543\n",
      "F1 Score: 0.7690567021279554\n",
      "--------------------------------------------------------\n",
      "ROC_AUC Score: 0.6679998723314267\n"
     ]
    }
   ],
   "source": [
    "# Metrics Word2Vec\n",
    "display_metrics(prediction_raw.select(col(label_col).alias('label'), col('prediction'), col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table: [[3017, 587], [0, 0]]\n",
      "Statistic=0.0, p-value=3.9484127069845653e-177\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "\n",
    "combined = pred_tfidf.join(pred_w2v, on=[text_col, label_col])\n",
    "# Convert to numpy arrays\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# contingency counts directly with boolean masks\n",
    "n11 = combined.filter((col(\"pred_tfidf\") == col(label_col)) & (col(\"pred_w2v\") == col(label_col))).count()\n",
    "n10 = combined.filter((col(\"pred_tfidf\") == col(label_col)) & (col(\"pred_w2v\") != col(label_col))).count()\n",
    "n01 = combined.filter((col(\"pred_tfidf\") != col(label_col)) & (col(\"pred_w2v\") == col(label_col))).count()\n",
    "n00 = combined.filter((col(\"pred_tfidf\") != col(label_col)) & (col(\"pred_w2v\") != col(label_col))).count()\n",
    "\n",
    "table = [[n11, n10],\n",
    "         [n01, n00]]\n",
    "print(\"Contingency Table:\", table)\n",
    "\n",
    "# --- McNemar‚Äôs test ---\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f\"Statistic={result.statistic}, p-value={result.pvalue}\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90d24f",
   "metadata": {},
   "source": [
    "X^2 (statistic) = (b-c)^2 / b + c <br>\n",
    "The ratio squared difference between disagreed negatives and positives to the sum of the number of disagreements.\n",
    "\n",
    "|x|Test 2 positive|\tTest 2 negative\t|Row total|\n",
    "|---------------|-------------------|---------|--------|\n",
    "|Test 1 positive|  a  |  b  |  a + b  |\n",
    "|Test 1 negative|  c  |  d  |  c + d  |\n",
    "|Column total   |  a + c  |  b + d  |  N  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05207cd",
   "metadata": {},
   "source": [
    "||||\n",
    "|-------|------|------|\n",
    "| Null Hypothesis | H0 | p(b) = p(c) |\n",
    "| Alt Hypothesis | H1 | p(b) =/= p(c) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ba2cf",
   "metadata": {},
   "source": [
    "### Paired t-test\n",
    "Using 5x2 fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4c28eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.24.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.16.3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (1.16.3)\n",
      "Collecting numpy>=2.3.5 (from mlxtend)\n",
      "  Using cached numpy-2.3.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pandas>=2.3.3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (2.3.3)\n",
      "Collecting scikit-learn>=1.8.0 (from mlxtend)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting matplotlib>=3.10.8 (from mlxtend)\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: joblib>=1.5.2 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas>=2.3.3->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas>=2.3.3->mlxtend) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.10.8->mlxtend) (1.17.0)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn>=1.8.0->mlxtend)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading mlxtend-0.24.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 1.3/1.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 8.7 MB/s  0:00:00\n",
      "Downloading matplotlib-3.10.8-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 16.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 14.4 MB/s  0:00:00\n",
      "Using cached numpy-2.3.5-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.9/8.0 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/8.0 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 13.4 MB/s  0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, scikit-learn, matplotlib, mlxtend\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.3.4\n",
      "\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "    Uninstalling numpy-2.3.4:\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "      Successfully uninstalled numpy-2.3.4\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "  Attempting uninstall: matplotlib\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "    Found existing installation: matplotlib 3.10.7\n",
      "   ---------------- ----------------------- 2/5 [scikit-learn]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "    Uninstalling matplotlib-3.10.7:\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "      Successfully uninstalled matplotlib-3.10.7\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   ------------------------ --------------- 3/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   -------------------------------- ------- 4/5 [mlxtend]\n",
      "   ---------------------------------------- 5/5 [mlxtend]\n",
      "\n",
      "Successfully installed matplotlib-3.10.8 mlxtend-0.24.0 numpy-2.3.5 scikit-learn-1.8.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daniel J. Robertson\\Documents\\Classwork\\CIS 531\\Final\\political-toxicity-final-project-CIS731\\jupyter_env\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daniel J. Robertson\\Documents\\Classwork\\CIS 531\\Final\\political-toxicity-final-project-CIS731\\jupyter_env\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "! pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "160acd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splits(df, n_splits=5, test_fraction=0.5):\n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        # Randomize order each time\n",
    "        train, test = train_df.randomSplit([1-test_fraction, test_fraction], \n",
    "                                           seed=i)\n",
    "        splits.append((train, test))\n",
    "        # Swap train/test for the \"√ó2\" part\n",
    "        splits.append((test, train))\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de54f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies (tfidf,w2v): 0.7905817174515235 0.8382271468144045\n",
      "Accuracies (tfidf,w2v): 0.7944664031620553 0.8243929983060417\n",
      "Accuracies (tfidf,w2v): 0.7911111111111111 0.8283333333333334\n",
      "Accuracies (tfidf,w2v): 0.7843468468468469 0.8288288288288288\n",
      "Accuracies (tfidf,w2v): 0.7986614612381484 0.8388176240936978\n",
      "Accuracies (tfidf,w2v): 0.7779024116657319 0.823331463825014\n",
      "Accuracies (tfidf,w2v): 0.7878453038674034 0.8320441988950277\n",
      "Accuracies (tfidf,w2v): 0.7967157417893544 0.8284258210645526\n",
      "Accuracies (tfidf,w2v): 0.8047138047138047 0.8389450056116723\n",
      "Accuracies (tfidf,w2v): 0.7926421404682275 0.8216276477146043\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"toxic_score\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "diffs = []\n",
    "splits = make_splits(train_df, n_splits=5)\n",
    "\n",
    "for train_split, test_split in splits:\n",
    "    # Fit both models\n",
    "    modelA = pipeline_tfidf.fit(train_split)\n",
    "    modelB = pipeline_w2v.fit(train_split)\n",
    "\n",
    "    # Predict\n",
    "    predA = modelA.transform(test_split)\n",
    "    predB = modelB.transform(test_split)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accA = evaluator.evaluate(predA)\n",
    "    accB = evaluator.evaluate(predB)\n",
    "    print(\"Accuracies (tfidf,w2v):\", accA, accB)\n",
    "\n",
    "    # Store difference\n",
    "    diffs.append(accA - accB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c46aab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t statistic: -17.516\n",
      "p value: 0.000\n",
      "Reject null hypothesis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "diffs = np.array(diffs)\n",
    "mean_diff = diffs.mean()\n",
    "std_diff = diffs.std(ddof=1)\n",
    "n = len(diffs)\n",
    "\n",
    "t_stat = mean_diff / (std_diff / sqrt(n))\n",
    "\n",
    "# Two‚Äësided p‚Äëvalue (using scipy if allowed, or approximate)\n",
    "from scipy.stats import t\n",
    "p_val = 2 * (1 - t.cdf(abs(t_stat), df=n-1))\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"t statistic: {t_stat:.3f}\")\n",
    "print(f\"p value: {p_val:.3f}\")\n",
    "print(\"Reject null hypothesis\" if p_val < alpha else \"Fail to reject null hypothesis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ee8afa",
   "metadata": {},
   "source": [
    "### Q + A \n",
    "Here's some observations I made while testing these models. I confirmed my suspicions after consulting Copilot and Google sources on the matter as well as my own background knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d30b5",
   "metadata": {},
   "source": [
    "**Why do you use multiclass classification evaluator for the accuracy score?** <br>\n",
    "In PySpark, the binary classifier I have developed is essentially treated as a multiclass classifier for the purposes of accessing the accuracy, f1, precision, and recall scores, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb739390",
   "metadata": {},
   "source": [
    "**Why not do more than 5 splits for the 5x2 test?** <br>\n",
    "The originator of the framework determined that 5x2 is the best version because it creates enough observations for a t test to be valuable. And, if we did any more it might make the features sort of colinear which biases the results and always makes them significant. (therefore lending to type 1 error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1507f6",
   "metadata": {},
   "source": [
    "**Why does the McNemar test show no significance?** <br>\n",
    "Tf-IDF outperforms Word2Vec in a case where the training dataset is equivalent to the test dataset because the features are dead even and the hashing function captured them all. Word2Vec performs better on less training information because *similar* words will still be classed together, unlike under Tf-IDF as a kind of Counting vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686fb9e",
   "metadata": {},
   "source": [
    "**How should we interpret the McNemar test versus the t-test?**\n",
    "1. Reject the McNemar test. It is not useful because one model structurally outperforms in that particular situation. I needed to perform a data holdout. The test is functionally invalid because it is structurally unsound.\n",
    "2. Accept the t-test. It is better representative of the data situation and cross validation provides more observations than the McNemar test.\n",
    "3. Reject McNemar becaues it only focuses on significant disagreement but does not compare agreement. That means where there are drastic differences between two models, the test tends to fail. Also if they generally agree on the same points but equally disagree on the same points, then the test comes out as a wash. This may be useful in some cases but this would not happen in a t test with cross validation. McNemar is useful, just not the way I performed it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3566935",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
