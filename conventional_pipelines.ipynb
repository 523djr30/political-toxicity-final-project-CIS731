{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf0572b",
   "metadata": {},
   "source": [
    "# Political Toxicity: Conventional NLP Pipelines\n",
    "Using Apache Spark, the project should be able to take in CSV data from social media posts and classify them accurately as toxic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4bf7d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql import DataFrame\n",
    "spark = SparkSession.builder.appName('Test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e9248b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, when\n",
    "\"\"\"\n",
    "Maps the column from string values to double values.\n",
    "0.0 in all unknown types or cases\n",
    "\"\"\"\n",
    "def map_toxic_hard(df, col_name=\"toxic_hard\", new_col=\"toxic_score\"):\n",
    "    return df.withColumn(\n",
    "        new_col,\n",
    "        when(trim(lower(col(col_name))) == \"true\", 1)   # string TRUE\n",
    "        .when(trim(lower(col(col_name))) == \"false\", 0) # string FALSE\n",
    "        .otherwise(0)                                   # everything else\n",
    "    )\n",
    "\n",
    "def preprocessing_steps(df: DataFrame) -> DataFrame:\n",
    "    temp = map_toxic_hard(df, \"toxic_hard\", \"toxic_score\")\n",
    "    return temp.select('text','toxic_score') \\\n",
    "        .dropna(subset=['text'])\n",
    "\n",
    "\"\"\"\n",
    "Reads the CSV containing the training data at the given path \n",
    "and returns a spark dataframe.\n",
    "\"\"\"\n",
    "def read_train_data(path:str) -> DataFrame:\n",
    "    seed_df = (\n",
    "        spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .option(\"multiline\", True)\n",
    "            .csv(path,sep=',',ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True)\n",
    "    )\n",
    "    seed_df = preprocessing_steps(seed_df)\n",
    "    return seed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2569a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train/dataset.csv\"\n",
    "seed_df = read_train_data(train_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cc5d2",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15806555",
   "metadata": {},
   "source": [
    "For all the tweets we need to remove all tokens that are not useful to the analysis. Then we need to create simple lists of relevant tokens for the unsupervised BERT model to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3a66cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- toxic_score: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "639e324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#from pyspark.ml import Transformer\n",
    "#from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "71747329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline builder\n",
    "\n",
    "# ETL\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='cleaned_words')\n",
    "\n",
    "# Hash/Vectorize\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol='features')\n",
    "\n",
    "# Regression Estimation\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='toxic_score',\n",
    "        maxIter=10, regParam=0.001)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a4782aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA T500\n"
     ]
    }
   ],
   "source": [
    "# Check whether the GPU is open\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ddaaadb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_246d53add9dc"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" DEBUG\n",
    "# Print out all parameters for this stage\n",
    "print(\"LogisticRegression parameters:\")\n",
    "print(lr.explainParams())\n",
    "# If you want to check just the labelCol and featuresCol:\n",
    "print(\"Label column:\", lr.getLabelCol())\n",
    "print(\"Features column:\", lr.getFeaturesCol())\n",
    "\"\"\"\n",
    "# Now fit only on the required columns\n",
    "model = pipeline.fit(seed_df)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472527d7",
   "metadata": {},
   "source": [
    "## Predict on Test Data\n",
    "This will load a small test subset and confirm our predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "64e3a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"data/test/2pt_test.csv\"\n",
    "\n",
    "test_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(test_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "273fbbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- toxic_score_true: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "da61616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------------------------------------+\n",
      "|toxic_score_true|prediction|probability                               |\n",
      "+----------------+----------+------------------------------------------+\n",
      "|0.0             |0.0       |[0.9985773268381002,0.0014226731618998123]|\n",
      "|1.0             |1.0       |[9.038140940731217E-5,0.9999096185905927] |\n",
      "+----------------+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)\n",
    "predictions.select('toxic_score_true','prediction','probability').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3c5ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "16579c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='cleaned_words')\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='toxic_score',\n",
    "        maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,hashingTF,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "10b8e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------+----------+\n",
      "|toxic_score_true|probability                              |prediction|\n",
      "+----------------+-----------------------------------------+----------+\n",
      "|0.0             |[0.8884556385439323,0.11154436145606772] |0.0       |\n",
      "|1.0             |[0.031650691973346586,0.9683493080266534]|1.0       |\n",
      "+----------------+-----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='toxic_score')\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,parallelism=10,\n",
    "    seed=SEED,\n",
    "    evaluator=evaluator,\n",
    "    estimatorParamMaps=grid)\n",
    "train_df = read_train_data(train_data_path)\n",
    "\n",
    "model = tvs.fit(train_df)\n",
    "model.bestModel.transform(dataset=test_df)\\\n",
    "    .select('toxic_score_true','probability','prediction')\\\n",
    "    .show(truncate=False)\n",
    "    # 'probability' represents the 1/P probability of that class \n",
    "    # being the predicted class of this document.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b6f6d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.6995571644466337]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics\n",
    "# represents ROC AUC for each parameter setting tuple in the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773918d",
   "metadata": {},
   "source": [
    "### McNemar's Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e749d",
   "metadata": {},
   "source": [
    "Compares where models differ in classification to determine if there is a signifcant difference between them. If for example they both classify the same amount incorrectly then obviously there is no significant difference, but if one model is technically more accurate but they agree on most of the classifications, then the data will show the result is not significant enough to disprove the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd9ec3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data 80/20\n",
    "test_fraction = 0.2\n",
    "train, test = train_df.randomSplit([1-test_fraction, test_fraction], \n",
    "                                           seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa5ee7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model metrics\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def display_metrics(predictions):\n",
    "    # Assume 'predictions' DataFrame has:\n",
    "    #   - 'label' (0 or 1)\n",
    "    #   - 'prediction' (0 or 1)\n",
    "    #   - 'probability' (vector of probabilities, needed for ROC/AUC)\n",
    "\n",
    "    # --- Accuracy ---\n",
    "    acc_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    accuracy = acc_eval.evaluate(predictions)\n",
    "    print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    confusion = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "    confusion.show()\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # --- Precision, Recall, F1 ---\n",
    "    prec_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    rec_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    f1_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "\n",
    "    precision = prec_eval.evaluate(predictions)\n",
    "    recall = rec_eval.evaluate(predictions)\n",
    "    f1 = f1_eval.evaluate(predictions)\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # --- ROC AUC ---\n",
    "    auc_eval = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    roc_auc = auc_eval.evaluate(predictions)\n",
    "    print(\"ROC_AUC Score:\", roc_auc)\n",
    "\n",
    "    return (accuracy,precision,recall,f1,roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6cca988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataFrame\n",
    "text_col = 'text'\n",
    "label_col = 'toxic_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "86894ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Word2Vec\n",
    "# Preprocessing steps for all pipelines\n",
    "tokenizer = Tokenizer(inputCol=text_col, outputCol=\"words\")\n",
    "filter = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='filtered_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5e8925da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF model\n",
    "hashing_tf = HashingTF(inputCol=filter.getOutputCol(), outputCol='raw_features')\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"features\")\n",
    "lr_tfidf = LogisticRegression(featuresCol=idf.getOutputCol(), labelCol=label_col,\n",
    "        maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_tfidf = Pipeline(stages=[tokenizer,filter,hashing_tf,idf,lr_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "084bbabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='\"\"\"Non-islamic countries\"\"', toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='\"\"\"Now go', toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='\"\"\"Vaccines may get authorized for young kids in October.\"\" https://www.businessinsider.com/when-can-young-kids-get-vaccinated-timeline-2021-7\"', toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='\"\"\"Well', toxic_score=0, pred_tfidf=0.0),\n",
       " Row(text='\"\"\"What a nightmare\"\"    why not bring back the troop before we pulled out incompetent decisione bless the innocent\"', toxic_score=0, pred_tfidf=0.0)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict on whole corpus (no holdout)\n",
    "model_tfidf = pipeline_tfidf.fit(train)\n",
    "prediction_raw = model_tfidf.transform(test)\n",
    "pred_tfidf = prediction_raw.select(text_col, label_col, col(\"prediction\").alias(\"pred_tfidf\"))\n",
    "pred_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "53ddbe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7875166002656042\n",
      "--------------------------------------------------------\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  572|\n",
      "|    0|       1.0|   67|\n",
      "|    1|       0.0|   93|\n",
      "|    1|       1.0|   21|\n",
      "+-----+----------+-----+\n",
      "\n",
      "--------------------------------------------------------\n",
      "Precision: 0.7660566210891228\n",
      "Recall: 0.7875166002656042\n",
      "F1 Score: 0.7759602421269185\n",
      "--------------------------------------------------------\n",
      "ROC_AUC Score: 0.6655204129258995\n"
     ]
    }
   ],
   "source": [
    "# Metrics Word2Vec\n",
    "metrics_tfidf = display_metrics(prediction_raw.select(col(label_col).alias('label'), col('prediction'), col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "931e8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=0, inputCol=filter.getOutputCol(), outputCol=\"features\")\n",
    "lr_w2v = LogisticRegression(featuresCol=word2vec.getOutputCol(), labelCol=label_col)\n",
    "pipeline_w2v = Pipeline(stages=[tokenizer, filter, word2vec,lr_w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f408c3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='\"\"\"Non-islamic countries\"\"', toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='\"\"\"Now go', toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='\"\"\"Vaccines may get authorized for young kids in October.\"\" https://www.businessinsider.com/when-can-young-kids-get-vaccinated-timeline-2021-7\"', toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='\"\"\"Well', toxic_score=0, pred_w2v=0.0),\n",
       " Row(text='\"\"\"What a nightmare\"\"    why not bring back the troop before we pulled out incompetent decisione bless the innocent\"', toxic_score=0, pred_w2v=0.0)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict on WHOLE corpus (no holdout)\n",
    "model_w2v = pipeline_w2v.fit(train)\n",
    "prediction_raw = model_w2v.transform(test)\n",
    "pred_w2v = prediction_raw.select(text_col, label_col, col(\"prediction\").alias(\"pred_w2v\"))\n",
    "pred_w2v.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "882b30b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8472775564409031\n",
      "--------------------------------------------------------\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|  637|\n",
      "|    0|       1.0|    2|\n",
      "|    1|       0.0|  113|\n",
      "|    1|       1.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n",
      "--------------------------------------------------------\n",
      "Precision: 0.7712138114209828\n",
      "Recall: 0.8472775564409031\n",
      "F1 Score: 0.7809345954131822\n",
      "--------------------------------------------------------\n",
      "ROC_AUC Score: 0.5996417099085735\n"
     ]
    }
   ],
   "source": [
    "# Metrics Word2Vec\n",
    "metrics_w2v=display_metrics(prediction_raw.select(col(label_col).alias('label'), col('prediction'), col('probability')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8366e680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (0.14.6)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from statsmodels) (2.3.5)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from statsmodels) (1.16.3)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "45f5080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency Table: [[571, 22], [67, 93]]\n",
      "Statistic=22.0, p-value=1.899849174722082e-06\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "\n",
    "combined = pred_tfidf.join(pred_w2v, on=[text_col, label_col])\n",
    "# Convert to numpy arrays\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# contingency counts directly with boolean masks\n",
    "n11 = combined.filter((col(\"pred_tfidf\") == col(label_col)) & (col(\"pred_w2v\") == col(label_col))).count()\n",
    "n10 = combined.filter((col(\"pred_tfidf\") == col(label_col)) & (col(\"pred_w2v\") != col(label_col))).count()\n",
    "n01 = combined.filter((col(\"pred_tfidf\") != col(label_col)) & (col(\"pred_w2v\") == col(label_col))).count()\n",
    "n00 = combined.filter((col(\"pred_tfidf\") != col(label_col)) & (col(\"pred_w2v\") != col(label_col))).count()\n",
    "\n",
    "table = [[n11, n10],\n",
    "         [n01, n00]]\n",
    "print(\"Contingency Table:\", table)\n",
    "\n",
    "# --- McNemar’s test ---\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f\"Statistic={result.statistic}, p-value={result.pvalue}\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90d24f",
   "metadata": {},
   "source": [
    "X^2 (statistic) = (b-c)^2 / b + c <br>\n",
    "The ratio squared difference between disagreed negatives and positives to the sum of the number of disagreements.\n",
    "\n",
    "|x|Test 2 positive|\tTest 2 negative\t|Row total|\n",
    "|---------------|-------------------|---------|--------|\n",
    "|Test 1 positive|  a  |  b  |  a + b  |\n",
    "|Test 1 negative|  c  |  d  |  c + d  |\n",
    "|Column total   |  a + c  |  b + d  |  N  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05207cd",
   "metadata": {},
   "source": [
    "||||\n",
    "|-------|------|------|\n",
    "| Null Hypothesis | H0 | p(b) = p(c) |\n",
    "| Alt Hypothesis | H1 | p(b) =/= p(c) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ba2cf",
   "metadata": {},
   "source": [
    "### Paired t-test\n",
    "Using 5x2 fold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4c28eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: scipy>=1.16.3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (1.16.3)\n",
      "Requirement already satisfied: numpy>=2.3.5 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (2.3.5)\n",
      "Requirement already satisfied: pandas>=2.3.3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.8.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (1.8.0)\n",
      "Requirement already satisfied: matplotlib>=3.10.8 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (3.10.8)\n",
      "Requirement already satisfied: joblib>=1.5.2 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib>=3.10.8->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas>=2.3.3->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from pandas>=2.3.3->mlxtend) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.10.8->mlxtend) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from scikit-learn>=1.8.0->mlxtend) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "160acd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splits(df, n_splits=5, test_fraction=0.5):\n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        # Randomize order each time\n",
    "        train, test = train_df.randomSplit([1-test_fraction, test_fraction], \n",
    "                                           seed=i)\n",
    "        splits.append((train, test))\n",
    "        # Swap train/test for the \"×2\" part\n",
    "        splits.append((test, train))\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "de54f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies (tfidf,w2v): 0.7905817174515235 0.8365650969529086\n",
      "Accuracies (tfidf,w2v): 0.7944664031620553 0.8266516092603049\n",
      "Accuracies (tfidf,w2v): 0.7911111111111111 0.8305555555555556\n",
      "Accuracies (tfidf,w2v): 0.7843468468468469 0.8305180180180181\n",
      "Accuracies (tfidf,w2v): 0.7986614612381484 0.8388176240936978\n",
      "Accuracies (tfidf,w2v): 0.7779024116657319 0.8222097588334268\n",
      "Accuracies (tfidf,w2v): 0.7878453038674034 0.8287292817679558\n",
      "Accuracies (tfidf,w2v): 0.7967157417893544 0.8278595696489242\n",
      "Accuracies (tfidf,w2v): 0.8047138047138047 0.8417508417508418\n",
      "Accuracies (tfidf,w2v): 0.7926421404682275 0.8199554069119287\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"toxic_score\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "diffs = []\n",
    "splits = make_splits(train_df, n_splits=5)\n",
    "\n",
    "for train_split, test_split in splits:\n",
    "    # Fit both models\n",
    "    modelA = pipeline_tfidf.fit(train_split)\n",
    "    modelB = pipeline_w2v.fit(train_split)\n",
    "\n",
    "    # Predict\n",
    "    predA = modelA.transform(test_split)\n",
    "    predB = modelB.transform(test_split)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accA = evaluator.evaluate(predA)\n",
    "    accB = evaluator.evaluate(predB)\n",
    "    print(\"Accuracies (tfidf,w2v):\", accA, accB)\n",
    "\n",
    "    # Store difference\n",
    "    diffs.append(accA - accB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c46aab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t statistic: -18.739\n",
      "p value: 0.000\n",
      "Reject null hypothesis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "diffs = np.array(diffs)\n",
    "mean_diff = diffs.mean()\n",
    "std_diff = diffs.std(ddof=1)\n",
    "n = len(diffs)\n",
    "\n",
    "t_stat = mean_diff / (std_diff / sqrt(n))\n",
    "\n",
    "# Two‑sided p‑value (using scipy if allowed, or approximate)\n",
    "from scipy.stats import t\n",
    "p_val = 2 * (1 - t.cdf(abs(t_stat), df=n-1))\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"t statistic: {t_stat:.3f}\")\n",
    "print(f\"p value: {p_val:.3f}\")\n",
    "print(\"Reject null hypothesis\" if p_val < alpha else \"Fail to reject null hypothesis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ee8afa",
   "metadata": {},
   "source": [
    "### Q + A \n",
    "Here's some observations I made while testing these models. I confirmed my suspicions after consulting Copilot and Google sources on the matter as well as my own background knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d30b5",
   "metadata": {},
   "source": [
    "**Why do you use multiclass classification evaluator for the accuracy score?** <br>\n",
    "In PySpark, the binary classifier I have developed is essentially treated as a multiclass classifier for the purposes of accessing the accuracy, f1, precision, and recall scores, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb739390",
   "metadata": {},
   "source": [
    "**Why not do more than 5 splits for the 5x2 test?** <br>\n",
    "The originator of the framework determined that 5x2 is the best version because it creates enough observations for a t test to be valuable. And, if we did any more it might make the features sort of colinear which biases the results and always makes them significant. (therefore lending to type 1 error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1507f6",
   "metadata": {},
   "source": [
    "**Why does the McNemar test show no significance?** <br>\n",
    "Tf-IDF outperforms Word2Vec in a case where the training dataset is equivalent to the test dataset because the features are dead even and the hashing function captured them all. Word2Vec performs better on less training information because *similar* words will still be classed together, unlike under Tf-IDF as a kind of Counting vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686fb9e",
   "metadata": {},
   "source": [
    "**How should we interpret the McNemar test versus the t-test?**\n",
    "1. Reject the McNemar test. It is not useful because one model structurally outperforms in that particular situation. I needed to perform a data holdout. The test is functionally invalid because it is structurally unsound.\n",
    "2. Accept the t-test. It is better representative of the data situation and cross validation provides more observations than the McNemar test.\n",
    "3. Reject McNemar becaues it only focuses on significant disagreement but does not compare agreement. That means where there are drastic differences between two models, the test tends to fail. Also if they generally agree on the same points but equally disagree on the same points, then the test comes out as a wash. This may be useful in some cases but this would not happen in a t test with cross validation. McNemar is useful, just not the way I performed it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3566935",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "658e4b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tf-IDF</td>\n",
       "      <td>0.787517</td>\n",
       "      <td>0.766057</td>\n",
       "      <td>0.787517</td>\n",
       "      <td>0.775960</td>\n",
       "      <td>0.665520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w2v</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.771214</td>\n",
       "      <td>0.847278</td>\n",
       "      <td>0.780935</td>\n",
       "      <td>0.599642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model       Acc  Precision    Recall        f1   roc_auc\n",
       "0  Tf-IDF  0.787517   0.766057  0.787517  0.775960  0.665520\n",
       "1     w2v  0.847278   0.771214  0.847278  0.780935  0.599642"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Visualizations for Data Comparison\n",
    "# Convert to dataframe\n",
    "#  return (accuracy,confusion,precision,recall,f1,roc_auc)\n",
    "import pandas as pd\n",
    "metrics_tfidf_df = pd.DataFrame({\n",
    "    'model': 'Tf-IDF',\n",
    "    'Acc': metrics_tfidf[0],\n",
    "    'Precision': metrics_tfidf[1],\n",
    "    'Recall': metrics_tfidf[2],\n",
    "    'f1': metrics_tfidf[3],\n",
    "    'roc_auc': metrics_tfidf[4]\n",
    "}, index=[0])\n",
    "metrics_w2v_df = pd.DataFrame({\n",
    "    'model': 'w2v',\n",
    "    'Acc': metrics_w2v[0],\n",
    "    'Precision': metrics_w2v[1],\n",
    "    'Recall': metrics_w2v[2],\n",
    "    'f1': metrics_w2v[3],\n",
    "    'roc_auc': metrics_w2v[4]\n",
    "},  index=[1])\n",
    "metrics = pd.concat([metrics_tfidf_df,metrics_w2v_df])\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "adb06445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniel j. robertson\\documents\\classwork\\cis 531\\final\\political-toxicity-final-project-cis731\\jupyter_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "377a0cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = metrics.groupby(by='model').sum().T.plot(kind='bar',figsize=(10,5));\n",
    "\n",
    "# Set custom y-axis limits\n",
    "y_min = 0.5\n",
    "y_max = 1\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "plt.title(\"Conventional Model Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.savefig('Conventional_metrics_1_test.png',bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c724af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
